<h1 align="center">AVID-FP Object Store</h1>
<p align="center"><em>The first production-grade implementation of the AVID-FP protocolâ€”fault-tolerant, verifiable, and deployable in a single command.</em></p>

<p align="center">
  <a href="https://github.com/your-repo/distributed_object_store/actions/workflows/ci.yml">
    <img src="https://img.shields.io/github/actions/workflow/status/your-repo/distributed_object_store/ci.yml?label=CI&logo=github" alt="CI Status">
  </a>
  <a href="LICENSE">
    <img src="https://img.shields.io/github/license/your-repo/distributed_object_store.svg" alt="MIT License">
  </a>
  <img src="https://img.shields.io/badge/Go-1.23-blue?logo=go" alt="Go 1.23">
  <img src="https://img.shields.io/docker/image-size/your-repo/avid-fp-store/latest?logo=docker" alt="Docker Image">
</p>

---

## 1â€‚Project Overview â€”

The **AVID-FP Object Store** converts cutting-edge research on verifiable erasure-coded storage into a runnable microservice:

| Stage | What happens 
|-------|--------------|
| â‘  **Client slices the object** into *m* data + (*n â€“ m*) parity fragments via SIMD-accelerated Reedâ€“Solomon. | 1.5â€“1.7Ã— storage overhead instead of 3Ã— replication. |
| â‘¡ **Client builds an FPCC** (fingerprinted cross-checksum): SHA-256 + 64-bit homomorphic fingerprint per fragment. | Reader can prove integrity after downloading only *m* shards. |
| â‘¢ **Fragments + FPCC fan-out** to *n* identical storage nodes over gRPC. | No single point of failure; client remains stateless after upload. |
| â‘£ **Nodes gossip Echo â†’ Ready**; once any node sees â‰¥ 2*f + 1 Readies it commits. | Durability & agreement survive â‰¤ *f = n â€“ m* Byzantine nodes. |
| â‘¤ **Reader grabs shard 0** to learn FPCC, fetches the next *m â€“ 1* good shards, validates on the fly, and decodes. | Bandwidth-proportional reads; corruption is detected immediately. |
| â‘¥ **Prometheus + Grafana** track throughput, latency, GC, snapshots. | Operators get SRE-grade visibility with zero extra work. |

Slides: [AVID FP â€“ Store.pptx](AVID%20FP%20-%20Store.pptx) â€ƒ â€¢â€ƒ [Design Doc](Design_Document.pdf) â€ƒ â€¢â€ƒ [Full Report](AVID_FP_Report.pdf)

---

## 2â€‚ğŸ¯ Why AVID-FP?  

| | What it means | Why you care |
|--|--|--|
|  **Research â†’ Reality** | 3.6 k LoC of Go, 95 % unit coverage, end-to-end protocol. | You can _run_ the paper, not just read it. |
|  **Bullet-proof Integrity** | SHA-256 + homomorphic fingerprints guard each fragment. | Validates GiB objects after fetching *m* shards. |
|  **Extreme Resilience** | RS (m,n) + Bracha quorum tolerates *f = n â€“ m* bad nodes. | Survives crashes, omissions, or malicious peers. |
|  **Blistering Performance** | Up to **400 MB sâ»Â¹** writes; \< 5 % verification overhead. | High throughput _and_ cryptographic safety. |
|  **Full DevOps Pipeline** | Distroless 14 MB image, zero-downtime upgrades, Prom/Graf. | Deploy and observe in minutes. |
|  **Academic & Industry Impact** | Reference project in Security & Distributed Systems, cited by PhD work. | Battle-tested learning & research platform. | 

---

## 3â€‚Project Structure  

<details>
<summary>Click to expand tree</summary>

```text
.
â”œâ”€â”€ bin/               # static binaries (built)
â”œâ”€â”€ cmd/               # server & client entry points
â”œâ”€â”€ pkg/               # erasure, fingerprint, protocol, storage
â”œâ”€â”€ configs/           # YAML per node
â”œâ”€â”€ deploy/            # Prometheus + Grafana
â”œâ”€â”€ Images/            # architecture figures
â”‚   â”œâ”€â”€ Figure1.png
â”‚   â””â”€â”€ Figure2.png
â”œâ”€â”€ snapshots_host/    # example snapshot output
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md          # â† you are here
â”œâ”€â”€ Design_Document.pdf
â”œâ”€â”€ Test_Verification.pdf
â””â”€â”€ User_Manual.pdf


## 3  System Design & Architecture
### 3.1 High-level Flow  
![High-Level Design](Images/Figure1.png)

### 3.2 Write / Read Sequence (m = 3, n = 5)  
![Disperse + Retrieve Sequence](Images/Figure2.png)

Detailed rationale & component diagrams live in [`Design_Document.pdf`](Design_Document.pdf).

---

## 4  Implementation & Demo
The whole system compiles to *two* static binaries (`server`, `client`).  
Run a 5-node demo cluster and perform a write/read in < 30 s:

```bash
git clone https://github.com/your-repo/distributed_object_store.git
cd distributed_object_store

# build + launch 5 nodes, Prometheus & Grafana
docker compose up -d

# generate 100 MiB sample
dd if=/dev/urandom of=demo.bin bs=1M count=100

# disperse (m=3,n=5)
docker compose cp demo.bin server1:/demo.bin
docker compose exec server1 /bin/client \
  -mode disperse -file /demo.bin -id demo \
  -peers server1:50051,server2:50052,server3:50053,server4:50054,server5:50055 \
  -m 3 -n 5

# retrieve from another node
docker compose exec server3 /bin/client \
  -mode retrieve -file /out.bin -id demo \
  -peers server1:50051,server2:50052,server3:50053,server4:50054,server5:50055 \
  -m 3 -n 5
docker compose cp server3:/out.bin .
diff demo.bin out.bin && echo "âœ… Integrity OK!"
```
Need more? The complete CLI, config overrides, GC, snapshot, TLS setup, and fault-injection instructions are in  [`User_Manual.pdf`](User_Manual.pdf).


---

## 5 Verification Suite
Formal verification document:  [`Test_Verification.pdf`](Test_Verification.pdf).
It covers ten scenariosâ€”happy path, availability, integrity breach, TLS, GC, snapshot, rolling upgradeâ€”and is executed automatically in CI via Docker-in-Docker.

Quick signal:
verification.sh / verification.ps1 wraps the entire suite; green exit = all guarantees upheld.

---

## 6 Project Accomplishments ğŸš€
| Achievement                     | Details                                                             |
| ------------------------------- | ------------------------------------------------------------------- |
| **First working AVID-FP**       | Theory â†’ code in 2 000 SLOC + 900 tests                             |
| **110 MBÂ·sâ»Â¹ sustained writes** | 3-of-5 cluster on a single laptop, < 6 % integrity overhead         |
| **Full Byzantine tolerance**    | Survives 2 crash/omission/corruption faults in 5-node demo          |
| **1-click DevOps**              | Distroless images, Compose up, Grafana dashboards, rolling upgrades |
| **Coverage & CI**               | >92 % unit coverage, matrix CI (TLS on/off, 3-of-5 & 4-of-6)        |
| **Community ready**             | MIT license, SBOM, docs, demo video                                 |

ğŸ¬ Watch the live demo:  [`Demo_Video`](Demo_Video.mp4).

---

## 7 Extra Goodies
Snapshots â€” run server -snapshot /backup to capture a crash-consistent archive.

Garbage Collection â€” configurable TTL (default = 24 h); GC loop purges expired objects automatically.

mTLS â€” one flag per node & client (-tls_cert, -tls_key, -tls_ca) secures gRPC.

Pluggable code â€” swap the Reedâ€“Solomon codec or fingerprint engine via Go interfaces (pkg/erasure, pkg/fingerprint).

Observability â€” Prometheus histograms (avid_fp_*), Grafana JSON pre-imported.

## 8 Future Roadmap
 Dynamic membership (Raft-backed peer registry)

 Streaming encode/decode for TB-scale objects

 Geo-replicated clusters (WAN-aware gossip)

 Local reconstruction codes (Azure LRC / Clay)

 OpenTelemetry tracing

## 9 Contributors & License
Author: Manoj Myneni
License: MIT â€” PRs & issue reports welcome!

## 10 Research Credits ğŸ™  
This project is a *practical* follow-up to  
> **James Hendricks, Gregory R. Ganger, Michael K. Reiter.**  *Verifying Distributed Erasure-Coded Data.* Carnegie Mellon University / UNC Chapel Hill, 2007.  

Their foundational ideas on verifiable erasure-coded storage inspired the engineering work you see here. 

## 11 Gratitude Message
Thanks to our Professor Anrin C. for constant help and motivation.

â€œStrong integrity, smart redundancyâ€”shipped in a 14 MB container.â€
